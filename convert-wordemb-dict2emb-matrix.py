"""
Takes as input a dictionary file and python pickle file containing a 
dictionary with pretrained word embeddings, and computes an initial encoding 
matrix W_emb based on these.

Usage example:

    python convert-wordemb-dict2emb-matrix.py tests/data/ttrain.dict.pkl WordEmb/D_german_50k_500k_168h.pkl --emb_dim 15 temb_pretrained
    python convert-wordemb-dict2emb-matrix.py Data/Training.dict.pkl WordEmb/D_german_50k_500k_168h.pkl --apply_spelling_corrections --emb_dim 300 OutMat

@author Iulian Vlad Serban
"""


import collections
import numpy
import operator
import os
import sys
import logging
import cPickle
import itertools
from collections import Counter
from utils import *

from sklearn.decomposition import PCA
from sklearn import preprocessing

from wordsegment import segment
import enchant
enchd = enchant.Dict("en_US")

alphabet = 'abcdefghijklmnopqrstuvwxyz'

def safe_pickle(obj, filename):
    if os.path.isfile(filename):
        logger.info("Overwriting %s." % filename)
    else:
        logger.info("Saving to %s." % filename)
   
def edits1(word):
    splits     = [(word[:i], word[i:]) for i in range(len(word) + 1)]
    deletes    = [a + b[1:] for a, b in splits if b]
    transposes = [a + b[1] + b[0] + b[2:] for a, b in splits if len(b)>1]
    replaces   = [a + c + b[1:] for a, b in splits for c in alphabet if b]
    inserts    = [a + c + b     for a, b in splits for c in alphabet]
    return set(deletes + transposes + replaces + inserts)


logging.basicConfig(level=logging.INFO)
logger = logging.getLogger('covert-wordemb-dict2emb-matrix')

import argparse
parser = argparse.ArgumentParser()
parser.add_argument("model_dictionary", type=str, help="Model dictionary generated by convert-text2dict.py")
parser.add_argument("embedding_dictionary", type=str, help="Python vocabulary (pkl file) which contains a dictionary for every word")
parser.add_argument("--emb_dim", type=int, default=300, help="Dimensionality of the generated word embedding matrix. PCA is performed to reduce dimensionality to this size.")
parser.add_argument("--std_dev", type=float, default=0.01, help="Standard deviation of the produced word embedding.")

parser.add_argument("--apply_spelling_corrections", action='store_false', help="If true, will apply spelling corrections to all words not found in the pretrained word embedding dictionary. These corrections are based on regex expressions and the enchant spelling corrector.")


parser.add_argument("output_matrix", type=str, help="Generated word embedding matrix (pkl file)")


args = parser.parse_args()

emb_dim = args.emb_dim
logger.info("Final word embedding dim: %d" % emb_dim)

std_dev = args.std_dev
logger.info("Final standard deviation: %f" % std_dev)

if not os.path.isfile(args.model_dictionary):
    raise Exception("Model dictionary file not found!")

if not os.path.isfile(args.embedding_dictionary):
    raise Exception("Embedding dictionary file not found!")


# Load model dictionary
model_dict = cPickle.load(open(args.model_dictionary, 'r'))

str_to_idx = dict([(tok, tok_id) for tok, tok_id, _, _ in model_dict])
i_dim = len(str_to_idx.keys())
logger.info("Vocabulary size: %d" % i_dim)

word_freq = dict([(tok_id, freq) for _, tok_id, freq, _ in model_dict])

# Load pretrained word embeddings
embedding_dict = cPickle.load(open(args.embedding_dictionary, "rb" ) )

raw_emb_dim = embedding_dict[embedding_dict.keys()[0]].shape[0]
logger.info("Raw word embedding dim: %d" % raw_emb_dim)

W_emb_raw = numpy.zeros((i_dim, raw_emb_dim))

words_found = 0
unique_words_left_out = []
unique_word_indices_left_out = []
word_freq_left_out = []
total_freq_left_out = 0
total_freq = 0

# Go through every word in the model dictionary and add the corresponding word embedding to W_emb_raw
for key in str_to_idx.iterkeys():
    index = str_to_idx[key]
    total_freq = total_freq + word_freq[index]
    if key in embedding_dict:
        W_emb_raw[index, :] = embedding_dict[key]
        words_found = words_found + 1
    elif key.title() in embedding_dict: # Check if word with capital first letter exists in word embedding dict
        W_emb_raw[index, :] = embedding_dict[key.title()]
        words_found = words_found + 1
    else:
        word_was_found = False
        if args.apply_spelling_corrections == True:
            # Many words contain accidental '-' symbols, check if there are valid words without them and between them
            if key.replace('-', '') in embedding_dict:
                W_emb_raw[index, :] = embedding_dict[key.replace('-', '')]
                words_found = words_found + 1
                word_was_found = True
            elif '-' in key:
                subwords_split = key.split('-')
                for subword in subwords_split:
                    if subword in embedding_dict:
                        W_emb_raw[index, :] = embedding_dict[subword]
                        words_found = words_found + 1
                        word_was_found = True
                        break

            # Last resort is to use a spelling checker to correct the word
            if word_was_found == False:
                suggestions = enchd.suggest(key)

                # Suggestions are only allowed to be two "edits" away from the original word
                allowed_suggestions = set(e2 for e1 in edits1(key) for e2 in edits1(e1))
                for suggestion in suggestions:
                    if (suggestion in allowed_suggestions) and (suggestion in embedding_dict):
                        W_emb_raw[index, :] = embedding_dict[suggestion]
                        words_found = words_found + 1
                        word_was_found = True
                        print 'Correcting ' + str(key) + ' -> ' + str(suggestion)
                        break

        if word_was_found == False:
            unique_words_left_out.append(key)
            unique_word_indices_left_out.append(index)
            word_freq_left_out.append(word_freq[index])
            total_freq_left_out = total_freq_left_out + word_freq[index]


unique_words_missing = i_dim - words_found
  
logger.info("Unique words found in model dictionary and word embeddings: %d" % words_found)
logger.info("Unique words left out: %d" % unique_words_missing)
logger.info("Terms in corpus: %d" % total_freq)
logger.info("Terms left out: %d" % total_freq_left_out)
logger.info("Percentage terms left out: %f" % (float(total_freq_left_out)/float(total_freq)))

print 'unique_words_left_out', unique_words_left_out

assert(raw_emb_dim >= emb_dim)

# Use PCA to reduce dimensionality appropriately
if raw_emb_dim > emb_dim:
    pca = PCA(n_components=emb_dim)
    W_emb = pca.fit_transform(W_emb_raw)
else: # raw_emb_dim == emb_dim:
    W_emb = W_emb_raw

# Initialize words without embeddings randomly
seed = 123456
rng = numpy.random.RandomState(seed)
randmat = NormalInit(rng, unique_words_missing, emb_dim)
for i in range(unique_words_missing):
    W_emb[unique_word_indices_left_out[i], :] = randmat[i, :]

# Set mean to zero and standard deviation to 0.01
W_emb = preprocessing.scale(W_emb, with_std=std_dev)

# Create mask matrix, which represents word embeddings not pretrained
W_emb_nonpretrained_mask = numpy.zeros((i_dim, emb_dim))
for i in range(unique_words_missing):
    W_emb_nonpretrained_mask[unique_word_indices_left_out[i], :] = 1

safe_pickle([W_emb, W_emb_nonpretrained_mask], args.output_matrix + ".pkl")
